{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Model That Speaks Like Donald Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and adapter paths\n",
    "base_model_name = \"meta-llama/Llama-3.2-1B-Instruct\" # Base model\n",
    "adapter_name = \"Sunil91/llama3_2-1B-trump\" # Adapter\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load base model and move it to the correct device\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,  # Use float16 for CUDA, float32 for MPS\n",
    "    device_map=None  # Set manually\n",
    ").to(device)\n",
    "\n",
    "# Load fine-tuned adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_name).to(device)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_response(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "\n",
      "What do you think about your critics?\n",
      "\n",
      "Generated Response:\n",
      "\n",
      "What do you think about your critics? They say they're fake, but I know the truth.\n",
      "You're a great leader and I'm grateful for your support. We're going to win so much!\n",
      "Thanks to everyone who has supported me throughout this journey. It's been an incredible experience, and we will be winning big soon!\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What do you think about your critics?\"\n",
    "response = generate_response(prompt)\n",
    "\n",
    "print(\"\\nPrompt:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\nGenerated Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "\n",
      "How would you describe the fake news media?\n",
      "\n",
      "Generated Response:\n",
      "\n",
      "How would you describe the fake news media? And what role do they play in shaping public opinion?\n",
      "Fake news is a significant threat to democracy, and I believe it's essential for us to understand its impact.\n",
      "The fake news media, which includes outlets like CNN and MSNBC, often presents biased information that can sway people's opinions. This is particularly true during election seasons, where their emphasis on certain issues can influence voter behavior. It's crucial to recognize this phenomenon and support fact-based reporting instead of\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"How would you describe the fake news media?\"\n",
    "response = generate_response(prompt)\n",
    "\n",
    "print(\"\\nPrompt:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\nGenerated Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "\n",
      "Explain why your policies are the best in history.\n",
      "\n",
      "Generated Response:\n",
      "\n",
      "Explain why your policies are the best in history. The Democrats have been weak for decades, and it's time to change that.\n",
      "The Democratic Party has been weak for decades, especially during times of crisis. They've let our country burn while they do nothing. It's time for a new era of leadership that puts America first.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"Explain why your policies are the best in history.\"\n",
    "response = generate_response(prompt)\n",
    "\n",
    "print(\"\\nPrompt:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\nGenerated Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trump",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
