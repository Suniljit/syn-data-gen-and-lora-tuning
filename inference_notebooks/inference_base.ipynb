{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Base Model: Llama 3.2 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468a2f241c5f462682cc2b9716c4baec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187148acc022477b98a304d17b80c55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b759f70f9548579667cdca94166b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88b899783a544a6878ae0f6eaa1ee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7a0c4b08424a8e88c34341e0d52159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14f208b7d7646a48a7bc1448c83daaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer name\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with 4-bit precision for efficiency (if your system supports it)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use float16 for better performance\n",
    "    device_map=\"auto\"  # Automatically assign model to available GPU/CPU\n",
    ")\n",
    "\n",
    "# Create pipeline for inference\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=200,  # Limit response length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_response(prompt):\n",
    "    response = llama_pipeline(prompt, max_new_tokens=200, do_sample=True, temperature=0.7)\n",
    "    return response[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "\n",
      "What do you think about your critics?\n",
      "\n",
      "Generated Response:\n",
      "\n",
      "What do you think about your critics? Do you think you are able to handle criticism?\n",
      "\n",
      "As I reflect on my past interactions with critics, I realize that I have had my fair share of dealing with negative feedback. At times, I have been harshly critical of my peers, colleagues, and even myself. However, over time, I have come to understand the value of constructive criticism.\n",
      "\n",
      "Constructive criticism is not about tearing someone down, but about helping them grow and improve. It's about acknowledging their strengths and weaknesses, and offering suggestions for improvement. When I receive criticism, I try to separate the facts from the emotions and focus on the issue at hand.\n",
      "\n",
      "One of the most significant lessons I've learned is that criticism is a two-way street. It's not always easy to receive, but it's often necessary for growth and development. I've come to realize that criticism can be a powerful tool for self-improvement, and that it's essential to listen to feedback from others in a constructive manner.\n",
      "\n",
      "In my personal and professional\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"What do you think about your critics?\"\n",
    "output = generate_response(prompt)\n",
    "\n",
    "print(\"\\nPrompt:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\nGenerated Response:\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "\n",
      "How would you describe the fake news media?\n",
      "\n",
      "Generated Response:\n",
      "\n",
      "How would you describe the fake news media? A mix of facts and fiction, or a true reflection of the society?\n",
      "The fake news media is a complex phenomenon that can be described in different ways depending on one's perspective. Some people see it as a reflection of the society, while others view it as a separate entity with its own agenda.\n",
      "\n",
      "From a societal perspective, the fake news media can be seen as a reflection of the society's values, biases, and interests. The media plays a significant role in shaping public opinion and influencing the way people think about certain issues. The way the media presents information can either reflect or distort the truth, depending on the media outlet's values and biases. For example, some media outlets may be more likely to present news in a way that is favorable to a particular ideology or agenda, while others may be more likely to present news in a way that is balanced and neutral.\n",
      "\n",
      "On the other hand, some people see the fake news media as a separate entity with its own agenda. They may view the media\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"How would you describe the fake news media?\"\n",
    "output = generate_response(prompt)\n",
    "\n",
    "print(\"\\nPrompt:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\nGenerated Response:\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "\n",
      "Explain why your policies are the best in history.\n",
      "\n",
      "Generated Response:\n",
      "\n",
      "Explain why your policies are the best in history. You can argue that you are the best by citing relevant examples from history, such as the original settlers of the United States, the construction of the Great Pyramid of Giza, or the creation of the first telephone. You can also provide personal anecdotes or quotes from influential figures to support your argument. Additionally, you can highlight the unique aspects of your policies that set them apart from others.\n",
      "\n",
      "Here's an example of how you could write a persuasive essay arguing that your policies are the best in history:\n",
      "\n",
      "**Title:** The Unrivaled Excellence of My Policies: Why I am the Best in History\n",
      "\n",
      "**Introduction:**\n",
      "\n",
      "As the founder of a new nation, I am proud to present to you my policies, which have stood the test of time and have shaped the course of history. From the original settlers of the United States to the construction of the Great Pyramid of Giza, my policies have been instrumental in building a nation that is unmatched in its success. In this essay, I will argue that my\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"Explain why your policies are the best in history.\"\n",
    "output = generate_response(prompt)\n",
    "\n",
    "print(\"\\nPrompt:\\n\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\nGenerated Response:\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trump",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
